#!/usr/bin/env python3
"""
FA-DOSA å·®åˆ†æµ‹è¯•æ¡†æ¶ - ç»ˆæå®ç°

æœ¬è„šæœ¬ç”¨äºå¯¹FA-DOSA DNNåŠ é€Ÿå™¨è®¾è®¡æ¡†æ¶è¿›è¡Œç³»ç»ŸåŒ–çš„å·®åˆ†æµ‹è¯•ã€‚
æ ¸å¿ƒæµ‹è¯•å“²å­¦ï¼šé€»è¾‘æ ¸å¿ƒï¼Œç‰©ç†æ— å…³ - ä¸“æ³¨äºéªŒè¯åˆ†ææ¨¡å‹åœ¨æ ¸å¿ƒé€»è¾‘å±‚é¢ä¸åº•å±‚ä»¿çœŸå™¨çš„ä¸€è‡´æ€§ã€‚

ä½œè€…: FA-DOSA Team
ç‰ˆæœ¬: 1.0
"""

import argparse
import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional
import pandas as pd


class TimeloopReportParser:
    """
    Timeloopä»¿çœŸæŠ¥å‘Šè§£æå™¨
    
    ä¸“èŒè´Ÿè´£è§£æ timeloop-mapper.stats.txt æ–‡ä»¶ï¼Œæå–å…³é”®çš„æ€§èƒ½æŒ‡æ ‡ã€‚
    """
    
    def __init__(self):
        self.parsed_data = {}
    
    def parse_stats_file(self, stats_file_path: Path) -> Dict[str, Any]:
        """
        è§£æTimeloopç»Ÿè®¡æ–‡ä»¶
        
        Args:
            stats_file_path: timeloop-mapper.stats.txtæ–‡ä»¶è·¯å¾„
            
        Returns:
            ç»“æ„åŒ–çš„è§£æç»“æœå­—å…¸
        """
        if not stats_file_path.exists():
            raise FileNotFoundError(f"Stats file not found: {stats_file_path}")
        
        with open(stats_file_path, 'r') as f:
            content = f.read()
        
        self.parsed_data = {
            'summary': {},
            'components': {}
        }
        
        # è§£ææ€»å‘¨æœŸæ•°
        self._parse_summary_stats(content)
        
        # è§£æç»„ä»¶çº§åˆ«çš„è®¿é—®ç»Ÿè®¡
        self._parse_component_stats(content)
        
        return self.parsed_data
    
    def _parse_summary_stats(self, content: str) -> None:
        """
        è§£æSummary Statså—ä¸­çš„æ€»å‘¨æœŸæ•°
        """
        # æŸ¥æ‰¾Summary Statséƒ¨åˆ†
        summary_pattern = r'Summary Stats\s*\n-+\s*\n(.*?)(?=\n\n|$)'
        summary_match = re.search(summary_pattern, content, re.DOTALL)
        
        if summary_match:
            summary_content = summary_match.group(1)
            # æŸ¥æ‰¾Cyclesè¡Œ
            cycles_pattern = r'Cycles:\s*(\d+)'
            cycles_match = re.search(cycles_pattern, summary_content)
            
            if cycles_match:
                self.parsed_data['summary']['cycles'] = int(cycles_match.group(1))
        else:
            # å¦‚æœæ²¡æ‰¾åˆ°Summary Statsï¼Œå°è¯•ç›´æ¥æœç´¢Cycles
            cycles_pattern = r'Cycles:\s*(\d+)'
            cycles_match = re.search(cycles_pattern, content)
            if cycles_match:
                self.parsed_data['summary']['cycles'] = int(cycles_match.group(1))
    
    def _parse_component_stats(self, content: str) -> None:
        """
        è§£æå„ä¸ªç»„ä»¶çš„è®¿é—®ç»Ÿè®¡ä¿¡æ¯
        """
        # ä½¿ç”¨æ­£ç¡®çš„Levelåˆ†éš”ç¬¦
        level_pattern = r'Level \d+\n-+\n(.*?)(?=Level \d+\n-+|Networks\n-+|Operational|Summary Stats|$)'
        level_matches = re.finditer(level_pattern, content, re.DOTALL)
        
        for level_match in level_matches:
            level_content = level_match.group(1)
            
            # æŸ¥æ‰¾ç»„ä»¶åç§°
            component_match = re.search(r'=== (\w+) ===', level_content)
            if not component_match:
                continue
                
            component_name = component_match.group(1)
            
            # æŸ¥æ‰¾STATSéƒ¨åˆ†
            stats_match = re.search(r'STATS\s*\n\s*-+\s*\n(.*)', level_content, re.DOTALL)
            if not stats_match:
                continue
                
            stats_content = stats_match.group(1)
            
            self.parsed_data['components'][component_name] = {
                'accesses': {}
            }
            
            # è§£ææ¯ä¸ªå¼ é‡ç±»å‹çš„è®¿é—®ä¿¡æ¯
            self._parse_tensor_accesses(stats_content, component_name)
    
    def _parse_tensor_accesses(self, component_content: str, component_name: str) -> None:
        """
        è§£æç»„ä»¶ä¸­æ¯ä¸ªå¼ é‡çš„è®¿é—®ç»Ÿè®¡
        """
        # æŸ¥æ‰¾å¼ é‡å— (Weights, Inputs, Outputs)
        tensor_pattern = r'(Weights|Inputs|Outputs):\s*\n(.*?)(?=\n\s*(?:Weights|Inputs|Outputs):|\n\nLevel|$)'
        tensor_matches = re.finditer(tensor_pattern, component_content, re.DOTALL)
        
        for tensor_match in tensor_matches:
            tensor_name = tensor_match.group(1)
            tensor_content = tensor_match.group(2)
            
            # åˆå§‹åŒ–å¼ é‡è®¿é—®æ•°æ®
            self.parsed_data['components'][component_name]['accesses'][tensor_name] = {
                'scalar_reads_total': 0.0,
                'scalar_fills_total': 0.0,
                'scalar_updates_total': 0.0
            }
            
            # è§£æå®ä¾‹æ•°é‡
            utilized_instances = self._extract_utilized_instances(tensor_content)
            
            # è§£ææ¯å®ä¾‹çš„è®¿é—®æ¬¡æ•°å¹¶è®¡ç®—æ€»æ•°
            reads_per_instance = self._extract_scalar_value(tensor_content, 'Scalar reads \\(per-instance\\)')
            fills_per_instance = self._extract_scalar_value(tensor_content, 'Scalar fills \\(per-instance\\)')
            updates_per_instance = self._extract_scalar_value(tensor_content, 'Scalar updates \\(per-instance\\)')
            
            # è®¡ç®—æ€»è®¿é—®æ¬¡æ•° = æ¯å®ä¾‹è®¿é—®æ¬¡æ•° Ã— å®ä¾‹æ•°é‡
            self.parsed_data['components'][component_name]['accesses'][tensor_name].update({
                'scalar_reads_total': reads_per_instance * utilized_instances,
                'scalar_fills_total': fills_per_instance * utilized_instances,
                'scalar_updates_total': updates_per_instance * utilized_instances
            })
            
            # æ–°å¢ï¼šæŠŠ per-instance å’Œ å®ä¾‹æ•°ä¹Ÿå­˜èµ·æ¥ï¼Œåé¢"atomic å¯¹é½"è¦ç”¨
            self.parsed_data['components'][component_name]['accesses'][tensor_name].update({
                'scalar_reads_per_instance': reads_per_instance,
                'scalar_fills_per_instance': fills_per_instance,
                'scalar_updates_per_instance': updates_per_instance,
                'utilized_instances': utilized_instances
            })
    
    def _extract_utilized_instances(self, tensor_content: str) -> float:
        """
        æå–Utilized instances (max)çš„å€¼
        """
        pattern = r'Utilized instances \(max\)\s*:\s*([\d.]+)'
        match = re.search(pattern, tensor_content)
        return float(match.group(1)) if match else 1.0
    
    def _extract_scalar_value(self, tensor_content: str, metric_name: str) -> float:
        """
        æå–æ ‡é‡å€¼ï¼ˆå¦‚reads, fills, updates per-instanceï¼‰
        """
        pattern = rf'{metric_name}\s*:\s*([\d.]+)'
        match = re.search(pattern, tensor_content)
        return float(match.group(1)) if match else 0.0


class DifferentialComparator:
    """
    å·®åˆ†æ¯”è¾ƒå™¨ - æµ‹è¯•æ¡†æ¶çš„çµé­‚
    
    å®ç°æ ¸å¿ƒçš„ã€èšç„¦äºé€»è¾‘è¡Œä¸ºçš„"å¯¹è´¦å¼•æ“"ã€‚
    """
    
    def __init__(self):
        # æ ¸å¿ƒæŒ‡æ ‡é…ç½®æ¸…å• - è¿™æ˜¯æ•´ä¸ªæµ‹è¯•æ¡†æ¶çš„"æ³•å¾‹"
        self.metrics_to_check = [
            {
                'name': 'Total Cycles',
                'dosa_path': ['calculation_summary', 'total_cycles'],
                'tl_path': ['summary', 'cycles'],
                'unit': 'cycles',
                'tolerance': 0.01
            },
            {
                'name': 'Atomic: L0 Reads (W)',
                'dosa_path': ['intra_level_consumption_trace', 'L0_Registers (i=0)', 'W', 'consumption_reads'],
                'tl_path': ['components', 'L0_Registers', 'accesses', 'Weights', 'scalar_reads_total'],
                'unit': 'accesses',
                'tolerance': 0.01
            },
            {
                'name': 'Atomic: L2 Reads (I)',
                'dosa_path': ['intra_level_consumption_trace', 'L2_Scratchpad (i=2)', 'I', 'consumption_reads'],
                'tl_path': ['components', 'L2_Scratchpad', 'accesses', 'Inputs', 'scalar_reads_total'],
                'unit': 'accesses',
                'tolerance': 0.05
            },
            {
                'name': 'Atomic: Output Updates',
                'dosa_path': ['intra_level_consumption_trace', 'L1_Accumulator (i=1)', 'O', 'consumption_updates'],
                'tl_path':   ['components', 'L1_Accumulator', 'accesses', 'Outputs', 'scalar_updates_total'],
                'unit': 'accesses', 
                'tolerance': 0.05
            },
            {
                'name': 'Traffic: DRAM->L2 (Fills, W)',
                'dosa_path': ['inter_level_fill_traffic_trace', 'L2_Scratchpad (i=2)', 'W', 'tensor_fill_accesses'],
                'tl_path': ['components', 'L2_Scratchpad', 'accesses', 'Weights', 'scalar_fills_total'],
                'unit': 'accesses',
                'tolerance': 0.05
            },
            {
                'name': 'Traffic: DRAM->L2 (Fills, I)',
                'dosa_path': ['inter_level_fill_traffic_trace', 'L2_Scratchpad (i=2)', 'I', 'tensor_fill_accesses'],
                'tl_path': ['components', 'L2_Scratchpad', 'accesses', 'Inputs', 'scalar_fills_total'],
                'unit': 'accesses',
                'tolerance': 0.30
            },
            {
                'name': 'Traffic: L2->L0 (Fills, W)',
                'dosa_path': ['inter_level_fill_traffic_trace', 'L0_Registers (i=0)', 'W', 'tensor_fill_accesses'],
                'tl_path': ['components', 'L0_Registers', 'accesses', 'Weights', 'scalar_fills_total'],
                'unit': 'accesses',
                'tolerance': 0.05
            }
        ]
    
    def compare_results(self, dosa_data: Dict[str, Any], timeloop_data: Dict[str, Any], vp_id: str) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œå·®åˆ†æ¯”è¾ƒ
        
        Args:
            dosa_data: FA-DOSAåˆ†ææ¨¡å‹æ•°æ®
            timeloop_data: Timeloopä»¿çœŸæ•°æ®
            vp_id: éªŒè¯ç‚¹ID
            
        Returns:
            å·®å¼‚è®°å½•åˆ—è¡¨
        """
        differences = []
        
        for metric in self.metrics_to_check:
            dosa_value = self._get_value(dosa_data, metric['dosa_path'])
            tl_value = self._get_value(timeloop_data, metric['tl_path'])
            
            # === ä¸“ç”¨åˆ†æ”¯ï¼šAtomic: Output Updatesï¼ˆæŒ‰è®ºæ–‡å£å¾„å¯¹é½ï¼‰ ===
            if metric.get('name') == 'Atomic: Output Updates':
                try:
                    # 1) ä» TL(L1) å– per-instance å’Œ utilized_instances
                    tl_L1_O = timeloop_data['components']['L1_Accumulator']['accesses']['Outputs']
                    per_inst_updates = float(tl_L1_O.get('scalar_updates_per_instance', 0.0))
                    utilized_insts   = float(tl_L1_O.get('utilized_instances', 1.0))
                    
                    # 2) ä» DOSA è°ƒè¯•é‡Œæ‹¿ F_S,O(L1) â€”â€” è¿™å°±æ˜¯"ä¸ O æ— å…³çš„ç©ºé—´å¹¶è¡Œ"ï¼Œç­‰ä»·äº C_spatialï¼ˆR/S/Nè‹¥æ— å¹¶è¡Œåˆ™ä¸º1ï¼‰
                    #    è¯¥å€¼åœ¨ä½ çš„ debug é‡Œæ˜¯ intra_level_consumption_trace çš„ "F_S,t(i)" å­—æ®µ
                    dosa_F_S_O_L1 = self._get_value(
                        dosa_data,
                        ['intra_level_consumption_trace', 'L1_Accumulator (i=1)', 'O', 'F_S,t(i)']
                    )
                    if dosa_F_S_O_L1 == 0.0:
                        dosa_F_S_O_L1 = 1.0  # å…œåº•ï¼Œé¿å…é™¤é›¶
                    
                    # 3) ç”± "L1 å®ä¾‹æ•° = C_spatial * K_spatial" å¾—åˆ° K_spatial
                    K_spatial = utilized_insts / dosa_F_S_O_L1
                    
                    # 4) TL çš„"atomic å¯¹é½å€¼" = per-instance Ã— K_spatial
                    tl_value = per_inst_updates * K_spatial
                except Exception as e:
                    # è§£æå¤±è´¥æ—¶ï¼Œä¿ç•™åŸ tl_valueï¼ˆé€šå¸¸æ˜¯æ€»æ•°ï¼‰ï¼Œä½†æ‰“ä¸ªæ ‡è®°æ›´å®¹æ˜“æ’æŸ¥
                    # ä½ ä¹Ÿå¯ä»¥é€‰æ‹©åœ¨ differences é‡ŒåŠ ä¸ª Note æé†’
                    pass
            # === ä¸“ç”¨åˆ†æ”¯ç»“æŸ ===
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºé›¶å®ˆå«æ¨¡å¼
            if metric.get('mode') == 'zero_guard':
                epsilon = float(metric.get('epsilon', 1.0))
                dosa_is_zero = abs(dosa_value) <= epsilon
                tl_is_zero = abs(tl_value) <= epsilon
                
                # å¦‚æœä¸¤ä¾§éƒ½æ¥è¿‘é›¶ï¼Œåˆ™è·³è¿‡ï¼ˆé€šè¿‡æµ‹è¯•ï¼‰
                if dosa_is_zero and tl_is_zero:
                    continue
                
                # å¦åˆ™è®°å½•å·®å¼‚
                differences.append({
                    'VP_ID': vp_id,
                    'Metric': metric['name'],
                    'DOSA_Value': dosa_value,
                    'Timeloop_Value': tl_value,
                    'Relative_Error': 'â€”',
                    'Tolerance': f'absâ‰¤{epsilon:g}',
                    'Unit': metric['unit'],
                    'Note': 'Expected zero under paper-B constraints'
                })
                continue
            
            # åŸæœ‰çš„æ¯”ç‡æ¨¡å¼é€»è¾‘
            # è®¡ç®—ç›¸å¯¹è¯¯å·®
            if tl_value == 0 and dosa_value == 0:
                relative_error = 0.0
            elif tl_value == 0:
                relative_error = float('inf')
            else:
                relative_error = abs(dosa_value - tl_value) / tl_value
            
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å®¹å¿åº¦é˜ˆå€¼
            if relative_error > metric['tolerance']:
                differences.append({
                    'VP_ID': vp_id,
                    'Metric': metric['name'],
                    'DOSA_Value': dosa_value,
                    'Timeloop_Value': tl_value,
                    'Relative_Error': f"{relative_error:.2%}",
                    'Tolerance': f"{metric['tolerance']:.2%}",
                    'Unit': metric['unit']
                })
        
        return differences
    
    def _get_value(self, data: Dict[str, Any], path: List[str]) -> float:
        """
        æ ¹æ®è·¯å¾„ä»åµŒå¥—å­—å…¸ä¸­å®‰å…¨åœ°æå–æ•°å€¼
        
        Args:
            data: æ•°æ®å­—å…¸
            path: è®¿é—®è·¯å¾„åˆ—è¡¨
            
        Returns:
            æå–çš„æ•°å€¼ï¼Œå¦‚æœè·¯å¾„ä¸å­˜åœ¨åˆ™è¿”å›0.0
        """
        # å¤„ç†ç‰¹æ®Šçš„ '__ZERO__' è·¯å¾„
        if path == ['__ZERO__']:
            return 0.0
            
        current = data
        
        try:
            for key in path:
                current = current[key]
            return float(current) if current is not None else 0.0
        except (KeyError, TypeError, ValueError):
            return 0.0


class DiffTestRunner:
    """
    å·®åˆ†æµ‹è¯•è¿è¡Œå™¨
    
    è´Ÿè´£è°ƒåº¦å’Œç¼–æ’æ•´ä¸ªæµ‹è¯•å·¥ä½œæµã€‚
    """
    
    def __init__(self, input_dir: Path):
        self.input_dir = input_dir
        self.parser = TimeloopReportParser()
        self.comparator = DifferentialComparator()
    
    def run_differential_test(self) -> None:
        """
        æ‰§è¡Œå®Œæ•´çš„å·®åˆ†æµ‹è¯•æµç¨‹
        """
        print("FA-DOSA å·®åˆ†æµ‹è¯•æ¡†æ¶å¯åŠ¨...")
        print(f"è¾“å…¥ç›®å½•: {self.input_dir}")
        
        # è‡ªåŠ¨å‘ç°æ–‡ä»¶å¯¹
        file_pairs = self._discover_file_pairs()
        
        if not file_pairs:
            print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ–‡ä»¶å¯¹è¿›è¡Œæµ‹è¯•")
            return
        
        print(f"å‘ç° {len(file_pairs)} ä¸ªéªŒè¯ç‚¹")
        
        all_differences = []
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶å¯¹
        for vp_id, dosa_file, timeloop_file in file_pairs:
            print(f"\nå¤„ç†éªŒè¯ç‚¹: {vp_id}")
            
            try:
                # è§£æDOSAæ•°æ®
                with open(dosa_file, 'r') as f:
                    dosa_data = json.load(f)
                
                # è§£æTimeloopæ•°æ®
                timeloop_data = self.parser.parse_stats_file(timeloop_file)
                
                # æ‰§è¡Œæ¯”è¾ƒ
                differences = self.comparator.compare_results(dosa_data, timeloop_data, vp_id)
                all_differences.extend(differences)
                
                if differences:
                    print(f"  å‘ç° {len(differences)} ä¸ªå·®å¼‚")
                else:
                    print("  âœ“ æ‰€æœ‰æŒ‡æ ‡å‡åœ¨å®¹å¿åº¦èŒƒå›´å†…")
                    
            except Exception as e:
                print(f"  é”™è¯¯: å¤„ç†éªŒè¯ç‚¹ {vp_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        
        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_final_report(all_differences)
    
    def _discover_file_pairs(self) -> List[Tuple[str, Path, Path]]:
        """
        è‡ªåŠ¨å‘ç°æˆå¯¹çš„DOSAå’ŒTimeloopæ–‡ä»¶
        
        Returns:
            (vp_id, dosa_file_path, timeloop_file_path) çš„åˆ—è¡¨
        """
        file_pairs = []
        
        # æŸ¥æ‰¾æ‰€æœ‰DOSAè°ƒè¯•æ–‡ä»¶
        dosa_pattern = "debug_performance_model_point_*.json"
        # é¦–å…ˆåœ¨æ ¹ç›®å½•æŸ¥æ‰¾
        dosa_files = list(self.input_dir.glob(dosa_pattern))
        # å¦‚æœæ ¹ç›®å½•æ²¡æœ‰ï¼Œåˆ™åœ¨outputç›®å½•æŸ¥æ‰¾
        if not dosa_files:
            output_dir = self.input_dir / "output"
            if output_dir.exists():
                dosa_files = list(output_dir.glob(dosa_pattern))
        
        for dosa_file in dosa_files:
            # æå–éªŒè¯ç‚¹ID
            match = re.search(r'debug_performance_model_point_(\d+)\.json$', dosa_file.name)
            if not match:
                continue
            
            vp_id = match.group(1)
            
            # æŸ¥æ‰¾å¯¹åº”çš„Timeloopæ–‡ä»¶
            # å¦‚æœDOSAæ–‡ä»¶åœ¨outputç›®å½•ï¼Œåˆ™Timeloopæ–‡ä»¶ä¹Ÿåœ¨outputç›®å½•
            if "output" in str(dosa_file.parent):
                timeloop_file = dosa_file.parent / f"validation_workspace_{vp_id}" / "timeloop-mapper.stats.txt"
            else:
                timeloop_file = self.input_dir / "output" / f"validation_workspace_{vp_id}" / "timeloop-mapper.stats.txt"
            
            if timeloop_file.exists():
                file_pairs.append((vp_id, dosa_file, timeloop_file))
            else:
                print(f"è­¦å‘Š: éªŒè¯ç‚¹ {vp_id} ç¼ºå°‘Timeloopç»Ÿè®¡æ–‡ä»¶")
        
        return sorted(file_pairs, key=lambda x: int(x[0]))
    
    def _generate_final_report(self, all_differences: List[Dict[str, Any]]) -> None:
        """
        ç”Ÿæˆæœ€ç»ˆçš„å·®å¼‚æŠ¥å‘Š
        """
        print("\n" + "="*80)
        print("FA-DOSA å·®åˆ†æµ‹è¯•ç»“æœæŠ¥å‘Š")
        print("="*80)
        
        if not all_differences:
            print("\nğŸ‰ æµ‹è¯•é€šè¿‡ï¼")
            print("æ‰€æœ‰éªŒè¯ç‚¹çš„æ ¸å¿ƒé€»è¾‘æŒ‡æ ‡å‡ä¸Timeloopä»¿çœŸå™¨ä¿æŒä¸€è‡´ã€‚")
            print("åˆ†ææ¨¡å‹åœ¨é€»è¾‘å±‚é¢çš„è¡Œä¸ºå·²é€šè¿‡éªŒè¯ã€‚")
        else:
            print(f"\nâš ï¸  å‘ç° {len(all_differences)} ä¸ªå·®å¼‚éœ€è¦å…³æ³¨")
            
            # åˆ›å»ºDataFrameå¹¶æ ¼å¼åŒ–è¾“å‡º
            df = pd.DataFrame(all_differences)
            
            # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåºï¼Œå¦‚æœå­˜åœ¨Noteåˆ—åˆ™åŒ…å«å®ƒ
            column_order = ['VP_ID', 'Metric', 'DOSA_Value', 'Timeloop_Value', 'Relative_Error', 'Tolerance', 'Unit']
            if 'Note' in df.columns:
                column_order.append('Note')
            df = df[column_order]
            
            print("\nå·®å¼‚è¯¦æƒ…:")
            print(df.to_string(index=False, float_format='%.2f'))
            
            # ä¿å­˜åˆ°CSVæ–‡ä»¶
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_filename = f"diff_report_logical_{timestamp}.csv"
            csv_path = self.input_dir / csv_filename
            
            df.to_csv(csv_path, index=False)
            print(f"\nå·®å¼‚æŠ¥å‘Šå·²ä¿å­˜è‡³: {csv_path}")
        
        print("\næµ‹è¯•å®Œæˆã€‚")


def main():
    """
    ä¸»å‡½æ•° - å‘½ä»¤è¡Œå…¥å£ç‚¹
    """
    parser = argparse.ArgumentParser(
        description="FA-DOSA å·®åˆ†æµ‹è¯•æ¡†æ¶ - éªŒè¯åˆ†ææ¨¡å‹ä¸ä»¿çœŸå™¨çš„æ ¸å¿ƒé€»è¾‘ä¸€è‡´æ€§",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python run_diff_test.py /path/to/validation/output
  
æ³¨æ„:
  - input_dir åº”æŒ‡å‘ç”± run_dmt_validation.py ç”Ÿæˆçš„å®Œæ•´è¾“å‡ºç›®å½•
  - è¯¥ç›®å½•åº”åŒ…å« debug_performance_model_point_*.json æ–‡ä»¶
  - ä»¥åŠå¯¹åº”çš„ validation_workspace_*/timeloop-mapper.stats.txt æ–‡ä»¶
        """
    )
    
    parser.add_argument(
        'input_dir',
        type=Path,
        help='åŒ…å«éªŒè¯æ•°æ®çš„è¾“å…¥ç›®å½•è·¯å¾„'
    )
    
    args = parser.parse_args()
    
    # éªŒè¯è¾“å…¥ç›®å½•
    if not args.input_dir.exists():
        print(f"é”™è¯¯: è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {args.input_dir}")
        return 1
    
    if not args.input_dir.is_dir():
        print(f"é”™è¯¯: è¾“å…¥è·¯å¾„ä¸æ˜¯ç›®å½•: {args.input_dir}")
        return 1
    
    # åˆ›å»ºå¹¶è¿è¡Œæµ‹è¯•å™¨
    try:
        runner = DiffTestRunner(args.input_dir)
        runner.run_differential_test()
        return 0
    except Exception as e:
        print(f"è‡´å‘½é”™è¯¯: {e}")
        return 1


if __name__ == "__main__":
    exit(main())