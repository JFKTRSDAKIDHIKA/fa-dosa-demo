import torch
import torch.nn as nn
import torch.optim as optim
import math

# æ­£ç¡®çš„importè·¯å¾„
from dosa.performance_model import HighFidelityPerformanceModel
from dosa.mapping import FineGrainedMapping
from dosa.config import Config
from dosa.hardware_parameters import HardwareParameters

# =============== é…ç½®å¸¸é‡ ===============
# ä¼˜åŒ–å™¨é…ç½®
LEARNING_RATE = 2e-8  # å­¦ä¹ ç‡ï¼Œæ§åˆ¶å‚æ•°æ›´æ–°æ­¥é•¿
NUM_OPTIMIZATION_STEPS = 10  # ä¼˜åŒ–è¿­ä»£æ¬¡æ•°
MAPPING_PENALTY_WEIGHT = 1e8  # æ˜ å°„æ— æ•ˆæƒ©ç½šæƒé‡

# =============== å·¥å…·å‡½æ•° ===============
def print_mapping_parameters(mapping, title="Mappingå‚æ•°è¯¦æƒ…", show_projected=True):
    """
    æ‰“å°mappingå‚æ•°çš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        mapping: FineGrainedMappingå¯¹è±¡
        title: æ‰“å°æ ‡é¢˜
        show_projected: æ˜¯å¦æ˜¾ç¤ºæŠ•å½±åçš„ç¦»æ•£å€¼
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ {title}")
    print(f"{'='*60}")
    
    if show_projected:
        projected_factors = mapping.get_all_factors()
    
    for i, (name, param) in enumerate(mapping.named_parameters()):
        continuous_val = param.data.item()
        real_val = math.exp(continuous_val)
        
        print(f"\nğŸ“Œ å‚æ•° {i+1}: {name}")
        print(f"   ğŸ“Š è¿ç»­å€¼ (log): {continuous_val:.6f}")
        print(f"   ğŸ”¢ çœŸå®å€¼: {real_val:.6f}")
        
        if show_projected:
            # Extract dimension and level from parameter name
            parts = name.split('.')
            if len(parts) >= 3:
                dim = parts[2]  # e.g. 'K'
                level = parts[1]  # e.g. 'L0_Registers'
                factor_type = parts[3]  # e.g. 'temporal' or 'spatial'
                
                # Print corresponding projected discrete factor
                if dim in projected_factors and level in projected_factors[dim]:
                    projected_val = projected_factors[dim][level][factor_type].item()
                    print(f"   ğŸ¯ æŠ•å½±å€¼: {projected_val:.3f}")
    
    print(f"{'='*60}")

def print_optimization_step(step, latency, energy, penalty, loss, current_loss, best_loss):
    """
    æ‰“å°ä¼˜åŒ–æ­¥éª¤çš„ä¿¡æ¯
    
    Args:
        step: å½“å‰æ­¥æ•°
        latency, energy, penalty, loss: æ€§èƒ½æŒ‡æ ‡
        current_loss: å½“å‰æŸå¤±
        best_loss: å†å²æœ€ä¼˜æŸå¤±
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ä¼˜åŒ–æ­¥éª¤ {step+1}/{NUM_OPTIMIZATION_STEPS}")
    print(f"{'='*60}")
    
    # æ€§èƒ½æŒ‡æ ‡
    print(f"ğŸ” æ€§èƒ½æŒ‡æ ‡:")
    print(f"   â±ï¸  Latency: {latency.item():.6e}")
    print(f"   âš¡ Energy: {energy.item():.6e}")
    print(f"   ğŸ“ˆ Performance Loss: {(latency*energy).item():.6e}")
    
    # æƒ©ç½šé¡¹
    print(f"\nâš ï¸  æƒ©ç½šé¡¹:")
    print(f"   ğŸš« Mapping Invalid Penalty: {penalty.item():.6e}")
    
    # æ€»æŸå¤±
    print(f"\nğŸ¯ æ€»æŸå¤±: {loss.item():.6e}")
    
    # æœ€ä¼˜è§£çŠ¶æ€
    if current_loss < best_loss:
        print(f"ğŸ‰ å‘ç°æ–°çš„æœ€ä¼˜è§£ï¼Loss = {current_loss:.6e}")
    else:
        print(f"ğŸ“‹ å½“å‰æœ€ä¼˜: {best_loss:.6e}")
    
    print(f"{'='*60}")

def print_fusion_gradients(fusion_params, title="Fusionå‚æ•°æ¢¯åº¦è¯¦æƒ…"):
    """
    æ‰“å°fusionå‚æ•°çš„æ¢¯åº¦ä¿¡æ¯
    """
    print(f"\n{'='*60}")
    print(f"ğŸ” {title}")
    print(f"{'='*60}")
    
    if fusion_params is None:
        print("âŒ fusion_params ä¸º None")
        return
    
    for name, param in fusion_params.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            grad_values = param.grad.detach().cpu().numpy()
            current_val = param.data.clone()
            update_val = current_val - LEARNING_RATE * 10 * param.grad  # Using larger learning rate for fusion
            
            print(f"\nğŸ“Œ å‚æ•°: {name}")
            print(f"   ğŸ“Š æ¢¯åº¦èŒƒæ•°: {grad_norm:.8f}")
            print(f"   ğŸ”¢ æ¢¯åº¦å€¼: {grad_values}")
            print(f"   ğŸ“ æ¢¯åº¦å½¢çŠ¶: {param.grad.shape}")
            print(f"   ğŸ“ˆ å½“å‰å€¼: {current_val.detach().cpu().numpy()}")
            print(f"   ğŸ”„ æ›´æ–°åå€¼: {update_val.detach().cpu().numpy()}")
            
            # Print fusion probability changes if it's logits
            if 'logits' in name:
                current_prob = torch.sigmoid(current_val)
                updated_prob = torch.sigmoid(update_val)
                print(f"   ğŸ¯ Fusionæ¦‚ç‡å˜åŒ–: {current_prob.item():.4f} â†’ {updated_prob.item():.4f}")
        else:
            print(f"\nğŸ“Œ å‚æ•°: {name}")
            print(f"   âŒ æ¢¯åº¦ä¸º None (å¯èƒ½æœªå‚ä¸è®¡ç®—å›¾)")
    print(f"{'='*60}")


def print_parameter_gradients(mapping, learning_rate):
    """
    æ‰“å°å‚æ•°æ¢¯åº¦å’Œæ›´æ–°ä¿¡æ¯
    
    Args:
        mapping: FineGrainedMappingå¯¹è±¡
        learning_rate: å­¦ä¹ ç‡
    """
    print(f"\nğŸ”§ å‚æ•°æ¢¯åº¦å’Œæ›´æ–°ä¿¡æ¯:")
    print(f"{'â”€'*50}")
    
    for i, (name, param) in enumerate(mapping.named_parameters()):
        if param.grad is None:
            continue
            
        old_val = param.data.clone()
        grad_val = param.grad.clone()
        update_val = old_val - learning_rate * grad_val

        real_old = math.exp(old_val.item())
        real_update = math.exp(update_val.item())

        print(f"\nğŸ“Œ å‚æ•° {i+1}: {name}")
        print(f"   ğŸ“Š å½“å‰å€¼ (log): {old_val.item():.6f} â†’ çœŸå®å€¼: {real_old:.6f}")
        print(f"   ğŸ“‰ æ¢¯åº¦: {grad_val.item():.6f}")
        print(f"   ğŸ”„ æ›´æ–°å (log): {update_val.item():.6f} â†’ çœŸå®å€¼: {real_update:.6f}")

def print_fusion_parameters(fusion_params, graph, title="Fusionå‚æ•°è¯¦æƒ…"):
    """
    æ‰“å°fusionå‚æ•°çš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        fusion_params: fusionå‚æ•°å­—å…¸
        graph: è®¡ç®—å›¾å¯¹è±¡
        title: æ‰“å°æ ‡é¢˜
    """
    print(f"\n{'='*60}")
    print(f"ğŸ”— {title}")
    print(f"{'='*60}")
    
    if fusion_params is None or "fusion_logits" not in fusion_params:
        print("âš ï¸ æœªæ‰¾åˆ°fusionå‚æ•°")
        return
    
    fusion_logits = fusion_params["fusion_logits"]
    # ç¡®ä¿fusion_probså§‹ç»ˆæ˜¯1ç»´å¼ é‡ï¼Œé¿å…squeezeå¯¼è‡´çš„0ç»´é—®é¢˜
    if fusion_logits.dim() == 0:
        fusion_probs = torch.sigmoid(fusion_logits).unsqueeze(0)
    else:
        fusion_probs = torch.sigmoid(fusion_logits)
    
    print(f"\nğŸ“Š Fusion Groups æ€»æ•°: {len(graph.fusion_groups)}")
    
    for i, group in enumerate(graph.fusion_groups):
        if i < len(fusion_probs):
            logit_val = fusion_logits[i].item() if fusion_logits.dim() > 0 else fusion_logits.item()
            prob_val = fusion_probs[i].item() if fusion_probs.dim() > 0 else fusion_probs.item()
        else:
            logit_val = 0.0
            prob_val = 0.5
        
        print(f"\nğŸ”— Fusion Group {i+1}: {' â†’ '.join(group)}")
        print(f"   ğŸ“Š Logitå€¼: {logit_val:.6f}")
        print(f"   ğŸ¯ Fusionæ¦‚ç‡: {prob_val:.6f}")
        print(f"   ğŸ’¡ çŠ¶æ€: {'å¯ç”¨' if prob_val > 0.5 else 'ç¦ç”¨'}")
    
    print(f"{'='*60}")

def print_graph_structure(graph, title="è®¡ç®—å›¾ç»“æ„"):
    """
    æ‰“å°è®¡ç®—å›¾çš„ç»“æ„ä¿¡æ¯
    
    Args:
        graph: è®¡ç®—å›¾å¯¹è±¡
        title: æ‰“å°æ ‡é¢˜
    """
    print(f"\n{'='*60}")
    print(f"ğŸ—ï¸ {title}")
    print(f"{'='*60}")
    
    print(f"\nğŸ“‹ å±‚ä¿¡æ¯:")
    for layer_name, layer_info in graph.layers.items():
        print(f"   ğŸ”¸ {layer_name}: {layer_info['type']}")
        print(f"      è¾“å…¥å½¢çŠ¶: {layer_info['input_shape']}")
        print(f"      è¾“å‡ºå½¢çŠ¶: {layer_info['output_shape']}")
    
    print(f"\nğŸ”— èåˆç»„:")
    for i, group in enumerate(graph.fusion_groups):
        print(f"   Group {i+1}: {' â†’ '.join(group)}")
    
    print(f"\nğŸ“ å±‚æ‰§è¡Œé¡ºåº: {' â†’ '.join(graph.layer_order)}")
    print(f"{'='*60}")

def print_best_solution_summary(best_step, best_loss, best_metrics):
    """
    æ‰“å°æœ€ä¼˜è§£çš„æ‘˜è¦ä¿¡æ¯
    
    Args:
        best_step: æœ€ä¼˜è§£å‡ºç°çš„æ­¥æ•°
        best_loss: æœ€ä¼˜æŸå¤±å€¼
        best_metrics: æœ€ä¼˜è§£çš„è¯¦ç»†æŒ‡æ ‡
    """
    print(f"\n{'='*70}")
    print(f"ğŸ† ä¼˜åŒ–å®Œæˆï¼æœ€ä¼˜è§£æ‘˜è¦æŠ¥å‘Š")
    print(f"{'='*70}")
    
    print(f"ğŸ“ æœ€ä¼˜è§£æ¥æº: ç¬¬ {best_step+1} æ­¥")
    print(f"ğŸ¯ æœ€ä¼˜æŸå¤±: {best_loss:.6e}")
    
    print(f"\nğŸ“Š è¯¦ç»†æ€§èƒ½æŒ‡æ ‡:")
    print(f"   â±ï¸  Latency: {best_metrics['latency']:.6e}")
    print(f"   âš¡ Energy: {best_metrics['energy']:.6e}")
    print(f"   ğŸ“ Area: {best_metrics['area']:.6e}")
    print(f"   âš ï¸  Mapping Invalid Penalty: {best_metrics['mapping_invalid_penalty']:.6e}")
    print(f"   ğŸ¯ Total Penalty: {best_metrics['penalty']:.6e}")
    
    print(f"{'='*70}")

# def create_mock_graph(problem_dims):
#     """
#     åˆ›å»ºæ¨¡æ‹Ÿå›¾å¯¹è±¡
    
#     Args:
#         problem_dims: é—®é¢˜ç»´åº¦å­—å…¸
        
#     Returns:
#         MockGraphå¯¹è±¡
#     """
#     class MockGraph:
#         def __init__(self, dims):
#             self.problem_dims = dims
#             self.layers = {}
#             # åˆ›å»ºä¸€ä¸ªç®€å•çš„å·ç§¯å±‚ï¼ˆä½¿ç”¨å­—å…¸ç»“æ„ï¼‰
#             self.layers['conv1'] = {
#                 'type': 'Conv',
#                 'dims': dims,
#                 'input_shape': [dims['N'], dims['C'], dims['P'] + dims['R'] - 1, dims['Q'] + dims['S'] - 1],
#                 'output_shape': [dims['N'], dims['K'], dims['P'], dims['Q']],
#                 'weight_shape': [dims['K'], dims['C'], dims['R'], dims['S']]
#             }
#             self.fusion_groups = [['conv1']]  # å•å±‚èåˆç»„
#             self.layer_order = ['conv1']
#             self.adjacency = {}
    
#     return MockGraph(problem_dims)

def create_mock_graph(problem_dims):
    """
    Create a mock graph with simple Conv-BN-ReLU fusion
    """
    class MockGraph:
        def __init__(self, dims):
            self.problem_dims = dims
            self.layers = {}

            # Input layer
            self.layers['input'] = {
                'type': 'Input',
                'dims': dims,
                'input_shape': [dims['N'], dims['C'], dims['P'] + dims['R'] - 1, dims['Q'] + dims['S'] - 1],
                'output_shape': [dims['N'], dims['C'], dims['P'] + dims['R'] - 1, dims['Q'] + dims['S'] - 1]
            }

            # Conv-BN-ReLU fusion block
            self.layers['conv1'] = {
                'type': 'Conv',
                'dims': dims,
                'input_shape': [dims['N'], dims['C'], dims['P'] + dims['R'] - 1, dims['Q'] + dims['S'] - 1],
                'output_shape': [dims['N'], dims['K'], dims['P'], dims['Q']],
                'weight_shape': [dims['K'], dims['C'], dims['R'], dims['S']]
            }

            self.layers['bn1'] = {
                'type': 'BatchNormalization',
                'dims': dims,
                'input_shape': [dims['N'], dims['K'], dims['P'], dims['Q']],
                'output_shape': [dims['N'], dims['K'], dims['P'], dims['Q']]
            }

            self.layers['relu1'] = {
                'type': 'ReLU',
                'dims': dims,
                'input_shape': [dims['N'], dims['K'], dims['P'], dims['Q']],
                'output_shape': [dims['N'], dims['K'], dims['P'], dims['Q']]
            }

            # Output layer
            self.layers['output'] = {
                'type': 'Output',
                'dims': dims,
                'input_shape': [dims['N'], dims['K'], dims['P'], dims['Q']],
                'output_shape': [dims['N'], dims['K'], dims['P'], dims['Q']]
            }

            # Define single fusion group for Conv-BN-ReLU
            self.fusion_groups = [
                ['conv1', 'bn1', 'relu1']
            ]
            
            # Define layer execution order
            self.layer_order = [
                'input', 'conv1', 'bn1', 'relu1', 'output'
            ]
            
            self.adjacency = {}  # Empty for simple case

    return MockGraph(problem_dims)


# =============== ä¸»ç¨‹åºåˆå§‹åŒ– ===============
def initialize_system():
    """
    åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶
    
    è¯¥å‡½æ•°è´Ÿè´£åˆ›å»ºå’Œé…ç½®æ‰€æœ‰å¿…è¦çš„ç³»ç»Ÿç»„ä»¶ï¼ŒåŒ…æ‹¬ï¼š
    - é…ç½®ç®¡ç†å™¨
    - æ€§èƒ½æ¨¡å‹
    - æ˜ å°„å¯¹è±¡
    - ç¡¬ä»¶å‚æ•°
    - æ¨¡æ‹Ÿè®¡ç®—å›¾
    
    Returns:
        tuple: (config, perf_model, mapping, hw_params, graph)
            - config: é…ç½®ç®¡ç†å™¨å®ä¾‹
            - perf_model: é«˜ä¿çœŸæ€§èƒ½æ¨¡å‹å®ä¾‹
            - mapping: ç»†ç²’åº¦æ˜ å°„å¯¹è±¡
            - hw_params: ç¡¬ä»¶å‚æ•°å¯¹è±¡
            - graph: æ¨¡æ‹Ÿè®¡ç®—å›¾å¯¹è±¡
    """
    # 1. è·å–configå•ä¾‹å®ä¾‹
    config = Config.get_instance()

    # 2. åˆ›å»ºæ€§èƒ½æ¨¡å‹å®ä¾‹ï¼Œå¯ç”¨å»¶è¿Ÿè°ƒè¯•
    perf_model = HighFidelityPerformanceModel(config, debug_latency=True)

    # 3. æ„é€ ç®€å•çš„é—®é¢˜ç»´åº¦ï¼ˆæ¨¡æ‹Ÿå·ç§¯å±‚ï¼‰
    problem_dims = {
        "N": 1,    # batch size - æ‰¹æ¬¡å¤§å°
        "C": 64,   # input channels - è¾“å…¥é€šé“æ•°
        "K": 128,  # output channels - è¾“å‡ºé€šé“æ•°
        "P": 32,   # output height - è¾“å‡ºé«˜åº¦
        "Q": 32,   # output width - è¾“å‡ºå®½åº¦
        "R": 3,    # kernel height - å·ç§¯æ ¸é«˜åº¦
        "S": 3     # kernel width - å·ç§¯æ ¸å®½åº¦
    }

    # 4. åˆ›å»ºæ˜ å°„å¯¹è±¡
    mapping = FineGrainedMapping(problem_dims, config.MEMORY_HIERARCHY)

    # ä½¿ç”¨ç»éªŒè‰¯å¥½çš„èµ·å§‹ç‚¹åˆå§‹åŒ–æ˜ å°„å‚æ•°
    for i, p in enumerate(mapping.parameters()):
        if i == 0:  # æ—¶é—´æ˜ å°„å‚æ•°
            p.data.fill_(1.0)  # åå‘æ—¶é—´é‡ç”¨
        elif i == 1:  # ç©ºé—´æ˜ å°„å‚æ•°  
            p.data.fill_(2.0)  # é€‚åº¦ç©ºé—´å¹¶è¡Œ
        else:  # å†…å­˜å±‚æ¬¡å‚æ•°
            p.data.fill_(1.0)  # å¹³è¡¡å†…å­˜åˆ©ç”¨

    # 5. åˆ›å»ºç¡¬ä»¶å‚æ•°
    hw_params = HardwareParameters(
        initial_num_pes=16.0,   # åˆå§‹å¤„ç†å•å…ƒæ•°é‡
        initial_l0_kb=2.0,      # L0ç¼“å­˜å¤§å°(KB)
        initial_l1_kb=4.0,      # L1ç¼“å­˜å¤§å°(KB)
        initial_l2_kb=64.0      # L2ç¼“å­˜å¤§å°(KB)
    )

    # 6. åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡æ‹Ÿå›¾å¯¹è±¡
    graph = create_mock_graph(problem_dims)
    
    # 7. åˆ›å»ºfusionå‚æ•°ï¼ˆå¿…é¡»åœ¨graphåˆ›å»ºåï¼‰
    # ç¡®ä¿fusion_logitså§‹ç»ˆæ˜¯1ç»´å¼ é‡
    num_fusion_groups = len(graph.fusion_groups)
    fusion_params = nn.ParameterDict({
        "fusion_logits": nn.Parameter(torch.zeros(max(1, num_fusion_groups)))
    })
    
    return config, perf_model, mapping, hw_params, fusion_params, graph

# =============== ä¼˜åŒ–ä¸»å¾ªç¯ ===============
def run_optimization(perf_model, mapping, hw_params, fusion_params,graph):
    """
    è¿è¡Œä¼˜åŒ–å¾ªç¯
    
    è¯¥å‡½æ•°å®ç°äº†åŸºäºæ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–å¾ªç¯ï¼ŒåŒ…å«ä»¥ä¸‹ç‰¹æ€§ï¼š
    - Best-so-far ç­–ç•¥ï¼šè·Ÿè¸ªå¹¶ä¿å­˜å†å²æœ€ä¼˜è§£
    - è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡ç›‘æ§
    - å‚æ•°æ¢¯åº¦å’Œæ›´æ–°ä¿¡æ¯çš„å¯è§†åŒ–
    - è‡ªåŠ¨æ¢å¤æœ€ä¼˜è§£åˆ°æ˜ å°„å¯¹è±¡
    
    Args:
        perf_model: æ€§èƒ½æ¨¡å‹å®ä¾‹ï¼Œç”¨äºè®¡ç®—å»¶è¿Ÿã€èƒ½è€—ç­‰æŒ‡æ ‡
        mapping: æ˜ å°„å¯¹è±¡ï¼ŒåŒ…å«å¯ä¼˜åŒ–çš„æ˜ å°„å‚æ•°
        hw_params: ç¡¬ä»¶å‚æ•°å¯¹è±¡ï¼Œå®šä¹‰ç¡¬ä»¶é…ç½®
        graph: è®¡ç®—å›¾å¯¹è±¡ï¼Œæè¿°è¦ä¼˜åŒ–çš„ç¥ç»ç½‘ç»œç»“æ„
        
    Note:
        ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ä¿å­˜æœ€ä¼˜è§£ï¼Œå¹¶åœ¨ä¼˜åŒ–ç»“æŸåæ¢å¤åˆ°mappingå¯¹è±¡ä¸­
    """
    print("ğŸš€ å¼€å§‹æ€§èƒ½æ¨¡å‹ä¼˜åŒ–...")

    # åˆå§‹åŒ–ä¼˜åŒ–å™¨ - åˆ†åˆ«ä¸ºmappingå’Œfusionå‚æ•°åˆ›å»ºä¼˜åŒ–å™¨
    mapping_optimizer = optim.SGD(mapping.parameters(), lr=LEARNING_RATE)
    fusion_optimizer = optim.SGD(fusion_params.parameters(), lr=LEARNING_RATE * 10)  # fusionå‚æ•°ä½¿ç”¨æ›´å¤§çš„å­¦ä¹ ç‡

    # Best-so-far ç­–ç•¥åˆå§‹åŒ–
    best_loss = float('inf')        # å†å²æœ€ä¼˜æŸå¤±å€¼
    best_mapping_params = None      # æœ€ä¼˜æ˜ å°„å‚æ•°
    best_fusion_params = None       # æœ€ä¼˜fusionå‚æ•°
    best_step = -1                  # æœ€ä¼˜è§£å‡ºç°çš„æ­¥æ•°
    best_metrics = None             # æœ€ä¼˜è§£çš„è¯¦ç»†æŒ‡æ ‡

    # æ‰“å°åˆå§‹çŠ¶æ€
    print_graph_structure(graph, "åˆå§‹è®¡ç®—å›¾ç»“æ„")
    print_fusion_parameters(fusion_params, graph, "åˆå§‹Fusionå‚æ•°")
    # print_mapping_parameters(mapping, "åˆå§‹Mappingå‚æ•°")

    # ä¸»ä¼˜åŒ–å¾ªç¯
    for step in range(NUM_OPTIMIZATION_STEPS):
        mapping_optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
        fusion_optimizer.zero_grad()

        # å‰å‘ä¼ æ’­ï¼šè®¡ç®—æ€§èƒ½æŒ‡æ ‡
        latency, energy, area, mismatch, compat, mapping_invalid_penalty, penalty = perf_model(
            graph=graph,
            hw_params=hw_params,
            mapping=mapping,
            fusion_params=fusion_params
        )

        # è®¡ç®—æ€»æŸå¤±ï¼šæ€§èƒ½æŸå¤± + æ˜ å°„æ— æ•ˆæƒ©ç½š
        loss = (latency * energy) + MAPPING_PENALTY_WEIGHT * mapping_invalid_penalty
        current_loss = loss.item()
        
        # Best-so-far ç­–ç•¥ï¼šæ£€æŸ¥å¹¶æ›´æ–°æœ€ä¼˜è§£
        if current_loss < best_loss:
            best_loss = current_loss
            best_step = step
            # æ·±æ‹·è´å½“å‰æœ€ä¼˜çš„æ˜ å°„å‚æ•°
            best_mapping_params = {name: param.data.clone() for name, param in mapping.named_parameters()}
            # æ·±æ‹·è´å½“å‰æœ€ä¼˜çš„fusionå‚æ•°
            best_fusion_params = {name: param.data.clone() for name, param in fusion_params.named_parameters()}
            # ä¿å­˜æœ€ä¼˜è§£çš„è¯¦ç»†æŒ‡æ ‡
            best_metrics = {
                'latency': latency.item(),
                'energy': energy.item(),
                'area': area.item(),
                'mapping_invalid_penalty': mapping_invalid_penalty.item(),
                'penalty': penalty.item()
            }
            print(f"ğŸ¯ å‘ç°æ›´ä¼˜è§£ï¼æ­¥æ•°: {step}, æŸå¤±: {best_loss:.6f}")
            print_fusion_parameters(fusion_params, graph, f"æ­¥æ•° {step} - æœ€ä¼˜Fusionå‚æ•°")
        
        # æ‰“å°å½“å‰æ­¥éª¤çš„ä¼˜åŒ–ä¿¡æ¯
        print_optimization_step(step, latency, energy, penalty, loss, current_loss, best_loss)
        
        # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
        loss.backward()
        
        # æ‰“å°å‚æ•°æ¢¯åº¦å’Œæ›´æ–°ä¿¡æ¯
        # print_parameter_gradients(mapping, LEARNING_RATE)
        
        # æ‰“å°fusionå‚æ•°çš„æ¢¯åº¦ä¿¡æ¯
        print_fusion_gradients(fusion_params, f"Step {step} - Fusionå‚æ•°æ¢¯åº¦")
        
        # åˆ†åˆ«æ›´æ–°mappingå’Œfusionå‚æ•°
        mapping_optimizer.step()
        fusion_optimizer.step()
        
        # æ‰“å°æ›´æ–°åçš„mappingå‚æ•°
        # print_mapping_parameters(mapping, f"Step {step+1} - æ›´æ–°åçš„Mappingå‚æ•°")

    # ä¼˜åŒ–ç»“æŸåçš„æœ€ä¼˜è§£æ¢å¤
    if best_mapping_params is not None:
        # æ‰“å°æœ€ä¼˜è§£æ‘˜è¦
        # print_best_solution_summary(best_step, best_loss, best_metrics)
        
        # æ¢å¤æœ€ä¼˜å‚æ•°åˆ°mappingå¯¹è±¡
        for name, param in mapping.named_parameters():
            param.data.copy_(best_mapping_params[name])
        
        # æ¢å¤æœ€ä¼˜å‚æ•°åˆ°fusion_paramså¯¹è±¡
        if best_fusion_params is not None:
            for name, param in fusion_params.named_parameters():
                param.data.copy_(best_fusion_params[name])
        
        print(f"\nâœ… å·²å°†æœ€ä¼˜è§£å‚æ•°æ¢å¤åˆ°mappingå’Œfusionå¯¹è±¡ä¸­")
        
        # æ‰“å°æœ€ä¼˜è§£çš„è¯¦ç»†å‚æ•°ä¿¡æ¯
        # print_mapping_parameters(mapping, "ğŸ† æœ€ä¼˜è§£çš„è¯¦ç»†å‚æ•°", show_projected=True)
        print_fusion_parameters(fusion_params, graph, "ğŸ† æœ€ä¼˜è§£çš„Fusionå‚æ•°")
    else:
        print(f"\nâš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æœ€ä¼˜è§£")

# =============== ä¸»ç¨‹åºå…¥å£ ===============
def main():
    """
    ä¸»ç¨‹åºå…¥å£å‡½æ•°
    
    æ‰§è¡Œå®Œæ•´çš„æ€§èƒ½æ¨¡å‹ä¼˜åŒ–æµç¨‹ï¼š
    1. åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶
    2. è¿è¡Œä¼˜åŒ–å¾ªç¯
    3. è¾“å‡ºæœ€ç»ˆç»“æœ
    """
    print("ğŸ¯ DOSA æ€§èƒ½æ¨¡å‹ä¼˜åŒ–å¼€å§‹")
    print("=" * 70)
    
    # åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶
    config, perf_model, mapping, hw_params, fusion_params,graph = initialize_system()
    
    # è¿è¡Œä¼˜åŒ–
    run_optimization(perf_model, mapping, hw_params, fusion_params, graph)
    
    print("\nğŸ‰ DOSA æ€§èƒ½æ¨¡å‹ä¼˜åŒ–å®Œæˆ")
    print("=" * 70)

if __name__ == "__main__":
    main()
