"""
DOSA æ€§èƒ½æ¨¡å‹è°ƒè¯•è„šæœ¬

è¯¥è„šæœ¬ç”¨äºè°ƒè¯•å’Œä¼˜åŒ–æ·±åº¦å­¦ä¹ åŠ é€Ÿå™¨çš„æ€§èƒ½æ¨¡å‹ï¼Œå®ç°äº†ä»¥ä¸‹åŠŸèƒ½ï¼š
1. æ€§èƒ½æ¨¡å‹çš„å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
2. Best-so-far ä¼˜åŒ–ç­–ç•¥
3. è¯¦ç»†çš„å‚æ•°è·Ÿè¸ªå’Œæ—¥å¿—è¾“å‡º
4. æ˜ å°„å‚æ•°çš„å¯è§†åŒ–å’Œåˆ†æ

ä½œè€…: DOSA Team
ç‰ˆæœ¬: 1.0
"""

import torch
import torch.nn as nn
import torch.optim as optim
import math

# æ­£ç¡®çš„importè·¯å¾„
from dosa.performance_model import HighFidelityPerformanceModel
from dosa.mapping import FineGrainedMapping
from dosa.config import Config
from dosa.hardware_parameters import HardwareParameters

# =============== é…ç½®å¸¸é‡ ===============
# ä¼˜åŒ–å™¨é…ç½®
LEARNING_RATE = 2e-8  # å­¦ä¹ ç‡ï¼Œæ§åˆ¶å‚æ•°æ›´æ–°æ­¥é•¿
NUM_OPTIMIZATION_STEPS = 20  # ä¼˜åŒ–è¿­ä»£æ¬¡æ•°
MAPPING_PENALTY_WEIGHT = 1e8  # æ˜ å°„æ— æ•ˆæƒ©ç½šæƒé‡

# =============== å·¥å…·å‡½æ•° ===============
def print_mapping_parameters(mapping, title="Mappingå‚æ•°è¯¦æƒ…", show_projected=True):
    """
    æ‰“å°mappingå‚æ•°çš„è¯¦ç»†ä¿¡æ¯
    
    Args:
        mapping: FineGrainedMappingå¯¹è±¡
        title: æ‰“å°æ ‡é¢˜
        show_projected: æ˜¯å¦æ˜¾ç¤ºæŠ•å½±åçš„ç¦»æ•£å€¼
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ {title}")
    print(f"{'='*60}")
    
    if show_projected:
        projected_factors = mapping.get_all_factors()
    
    for i, (name, param) in enumerate(mapping.named_parameters()):
        continuous_val = param.data.item()
        real_val = math.exp(continuous_val)
        
        print(f"\nğŸ“Œ å‚æ•° {i+1}: {name}")
        print(f"   ğŸ“Š è¿ç»­å€¼ (log): {continuous_val:.6f}")
        print(f"   ğŸ”¢ çœŸå®å€¼: {real_val:.6f}")
        
        if show_projected:
            # Extract dimension and level from parameter name
            parts = name.split('.')
            if len(parts) >= 3:
                dim = parts[2]  # e.g. 'K'
                level = parts[1]  # e.g. 'L0_Registers'
                factor_type = parts[3]  # e.g. 'temporal' or 'spatial'
                
                # Print corresponding projected discrete factor
                if dim in projected_factors and level in projected_factors[dim]:
                    projected_val = projected_factors[dim][level][factor_type].item()
                    print(f"   ğŸ¯ æŠ•å½±å€¼: {projected_val:.3f}")
    
    print(f"{'='*60}")

def print_optimization_step(step, latency, energy, penalty, loss, current_loss, best_loss):
    """
    æ‰“å°ä¼˜åŒ–æ­¥éª¤çš„ä¿¡æ¯
    
    Args:
        step: å½“å‰æ­¥æ•°
        latency, energy, penalty, loss: æ€§èƒ½æŒ‡æ ‡
        current_loss: å½“å‰æŸå¤±
        best_loss: å†å²æœ€ä¼˜æŸå¤±
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ä¼˜åŒ–æ­¥éª¤ {step+1}/{NUM_OPTIMIZATION_STEPS}")
    print(f"{'='*60}")
    
    # æ€§èƒ½æŒ‡æ ‡
    print(f"ğŸ” æ€§èƒ½æŒ‡æ ‡:")
    print(f"   â±ï¸  Latency: {latency.item():.6e}")
    print(f"   âš¡ Energy: {energy.item():.6e}")
    print(f"   ğŸ“ˆ Performance Loss: {(latency*energy).item():.6e}")
    
    # æƒ©ç½šé¡¹
    print(f"\nâš ï¸  æƒ©ç½šé¡¹:")
    print(f"   ğŸš« Mapping Invalid Penalty: {penalty.item():.6e}")
    
    # æ€»æŸå¤±
    print(f"\nğŸ¯ æ€»æŸå¤±: {loss.item():.6e}")
    
    # æœ€ä¼˜è§£çŠ¶æ€
    if current_loss < best_loss:
        print(f"ğŸ‰ å‘ç°æ–°çš„æœ€ä¼˜è§£ï¼Loss = {current_loss:.6e}")
    else:
        print(f"ğŸ“‹ å½“å‰æœ€ä¼˜: {best_loss:.6e}")
    
    print(f"{'='*60}")

def print_parameter_gradients(mapping, learning_rate):
    """
    æ‰“å°å‚æ•°æ¢¯åº¦å’Œæ›´æ–°ä¿¡æ¯
    
    Args:
        mapping: FineGrainedMappingå¯¹è±¡
        learning_rate: å­¦ä¹ ç‡
    """
    print(f"\nğŸ”§ å‚æ•°æ¢¯åº¦å’Œæ›´æ–°ä¿¡æ¯:")
    print(f"{'â”€'*50}")
    
    for i, (name, param) in enumerate(mapping.named_parameters()):
        if param.grad is None:
            continue
            
        old_val = param.data.clone()
        grad_val = param.grad.clone()
        update_val = old_val - learning_rate * grad_val

        real_old = math.exp(old_val.item())
        real_update = math.exp(update_val.item())

        print(f"\nğŸ“Œ å‚æ•° {i+1}: {name}")
        print(f"   ğŸ“Š å½“å‰å€¼ (log): {old_val.item():.6f} â†’ çœŸå®å€¼: {real_old:.6f}")
        print(f"   ğŸ“‰ æ¢¯åº¦: {grad_val.item():.6f}")
        print(f"   ğŸ”„ æ›´æ–°å (log): {update_val.item():.6f} â†’ çœŸå®å€¼: {real_update:.6f}")

def print_best_solution_summary(best_step, best_loss, best_metrics):
    """
    æ‰“å°æœ€ä¼˜è§£çš„æ‘˜è¦ä¿¡æ¯
    
    Args:
        best_step: æœ€ä¼˜è§£å‡ºç°çš„æ­¥æ•°
        best_loss: æœ€ä¼˜æŸå¤±å€¼
        best_metrics: æœ€ä¼˜è§£çš„è¯¦ç»†æŒ‡æ ‡
    """
    print(f"\n{'='*70}")
    print(f"ğŸ† ä¼˜åŒ–å®Œæˆï¼æœ€ä¼˜è§£æ‘˜è¦æŠ¥å‘Š")
    print(f"{'='*70}")
    
    print(f"ğŸ“ æœ€ä¼˜è§£æ¥æº: ç¬¬ {best_step+1} æ­¥")
    print(f"ğŸ¯ æœ€ä¼˜æŸå¤±: {best_loss:.6e}")
    
    print(f"\nğŸ“Š è¯¦ç»†æ€§èƒ½æŒ‡æ ‡:")
    print(f"   â±ï¸  Latency: {best_metrics['latency']:.6e}")
    print(f"   âš¡ Energy: {best_metrics['energy']:.6e}")
    print(f"   ğŸ“ Area: {best_metrics['area']:.6e}")
    print(f"   âš ï¸  Mapping Invalid Penalty: {best_metrics['mapping_invalid_penalty']:.6e}")
    print(f"   ğŸ¯ Total Penalty: {best_metrics['penalty']:.6e}")
    
    print(f"{'='*70}")

def create_mock_graph(problem_dims):
    """
    åˆ›å»ºæ¨¡æ‹Ÿå›¾å¯¹è±¡
    
    Args:
        problem_dims: é—®é¢˜ç»´åº¦å­—å…¸
        
    Returns:
        MockGraphå¯¹è±¡
    """
    class MockGraph:
        def __init__(self, dims):
            self.problem_dims = dims
            self.layers = {}
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„å·ç§¯å±‚ï¼ˆä½¿ç”¨å­—å…¸ç»“æ„ï¼‰
            self.layers['conv1'] = {
                'type': 'Conv',
                'dims': dims,
                'input_shape': [dims['N'], dims['C'], dims['P'] + dims['R'] - 1, dims['Q'] + dims['S'] - 1],
                'output_shape': [dims['N'], dims['K'], dims['P'], dims['Q']],
                'weight_shape': [dims['K'], dims['C'], dims['R'], dims['S']]
            }
            self.fusion_groups = [['conv1']]  # å•å±‚èåˆç»„
            self.layer_order = ['conv1']
            self.adjacency = {}
    
    return MockGraph(problem_dims)

# =============== ä¸»ç¨‹åºåˆå§‹åŒ– ===============
def initialize_system():
    """
    åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶
    
    è¯¥å‡½æ•°è´Ÿè´£åˆ›å»ºå’Œé…ç½®æ‰€æœ‰å¿…è¦çš„ç³»ç»Ÿç»„ä»¶ï¼ŒåŒ…æ‹¬ï¼š
    - é…ç½®ç®¡ç†å™¨
    - æ€§èƒ½æ¨¡å‹
    - æ˜ å°„å¯¹è±¡
    - ç¡¬ä»¶å‚æ•°
    - æ¨¡æ‹Ÿè®¡ç®—å›¾
    
    Returns:
        tuple: (config, perf_model, mapping, hw_params, graph)
            - config: é…ç½®ç®¡ç†å™¨å®ä¾‹
            - perf_model: é«˜ä¿çœŸæ€§èƒ½æ¨¡å‹å®ä¾‹
            - mapping: ç»†ç²’åº¦æ˜ å°„å¯¹è±¡
            - hw_params: ç¡¬ä»¶å‚æ•°å¯¹è±¡
            - graph: æ¨¡æ‹Ÿè®¡ç®—å›¾å¯¹è±¡
    """
    # 1. è·å–configå•ä¾‹å®ä¾‹
    config = Config.get_instance()

    # 2. åˆ›å»ºæ€§èƒ½æ¨¡å‹å®ä¾‹ï¼Œå¯ç”¨å»¶è¿Ÿè°ƒè¯•
    perf_model = HighFidelityPerformanceModel(config, debug_latency=True)

    # 3. æ„é€ ç®€å•çš„é—®é¢˜ç»´åº¦ï¼ˆæ¨¡æ‹Ÿå·ç§¯å±‚ï¼‰
    problem_dims = {
        "N": 1,    # batch size - æ‰¹æ¬¡å¤§å°
        "C": 64,   # input channels - è¾“å…¥é€šé“æ•°
        "K": 128,  # output channels - è¾“å‡ºé€šé“æ•°
        "P": 32,   # output height - è¾“å‡ºé«˜åº¦
        "Q": 32,   # output width - è¾“å‡ºå®½åº¦
        "R": 3,    # kernel height - å·ç§¯æ ¸é«˜åº¦
        "S": 3     # kernel width - å·ç§¯æ ¸å®½åº¦
    }

    # 4. åˆ›å»ºæ˜ å°„å¯¹è±¡
    mapping = FineGrainedMapping(problem_dims, config.MEMORY_HIERARCHY)

    # ä½¿ç”¨ç»éªŒè‰¯å¥½çš„èµ·å§‹ç‚¹åˆå§‹åŒ–æ˜ å°„å‚æ•°
    for i, p in enumerate(mapping.parameters()):
        if i == 0:  # æ—¶é—´æ˜ å°„å‚æ•°
            p.data.fill_(1.0)  # åå‘æ—¶é—´é‡ç”¨
        elif i == 1:  # ç©ºé—´æ˜ å°„å‚æ•°  
            p.data.fill_(2.0)  # é€‚åº¦ç©ºé—´å¹¶è¡Œ
        else:  # å†…å­˜å±‚æ¬¡å‚æ•°
            p.data.fill_(1.0)  # å¹³è¡¡å†…å­˜åˆ©ç”¨

    # 5. åˆ›å»ºç¡¬ä»¶å‚æ•°
    hw_params = HardwareParameters(
        initial_num_pes=16.0,   # åˆå§‹å¤„ç†å•å…ƒæ•°é‡
        initial_l0_kb=2.0,      # L0ç¼“å­˜å¤§å°(KB)
        initial_l1_kb=4.0,      # L1ç¼“å­˜å¤§å°(KB)
        initial_l2_kb=64.0      # L2ç¼“å­˜å¤§å°(KB)
    )

    # 6. åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡æ‹Ÿå›¾å¯¹è±¡
    graph = create_mock_graph(problem_dims)
    
    return config, perf_model, mapping, hw_params, graph

# =============== ä¼˜åŒ–ä¸»å¾ªç¯ ===============
def run_optimization(perf_model, mapping, hw_params, graph):
    """
    è¿è¡Œä¼˜åŒ–å¾ªç¯
    
    è¯¥å‡½æ•°å®ç°äº†åŸºäºæ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–å¾ªç¯ï¼ŒåŒ…å«ä»¥ä¸‹ç‰¹æ€§ï¼š
    - Best-so-far ç­–ç•¥ï¼šè·Ÿè¸ªå¹¶ä¿å­˜å†å²æœ€ä¼˜è§£
    - è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡ç›‘æ§
    - å‚æ•°æ¢¯åº¦å’Œæ›´æ–°ä¿¡æ¯çš„å¯è§†åŒ–
    - è‡ªåŠ¨æ¢å¤æœ€ä¼˜è§£åˆ°æ˜ å°„å¯¹è±¡
    
    Args:
        perf_model: æ€§èƒ½æ¨¡å‹å®ä¾‹ï¼Œç”¨äºè®¡ç®—å»¶è¿Ÿã€èƒ½è€—ç­‰æŒ‡æ ‡
        mapping: æ˜ å°„å¯¹è±¡ï¼ŒåŒ…å«å¯ä¼˜åŒ–çš„æ˜ å°„å‚æ•°
        hw_params: ç¡¬ä»¶å‚æ•°å¯¹è±¡ï¼Œå®šä¹‰ç¡¬ä»¶é…ç½®
        graph: è®¡ç®—å›¾å¯¹è±¡ï¼Œæè¿°è¦ä¼˜åŒ–çš„ç¥ç»ç½‘ç»œç»“æ„
        
    Note:
        ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¼šè‡ªåŠ¨ä¿å­˜æœ€ä¼˜è§£ï¼Œå¹¶åœ¨ä¼˜åŒ–ç»“æŸåæ¢å¤åˆ°mappingå¯¹è±¡ä¸­
    """
    print("ğŸš€ å¼€å§‹æ€§èƒ½æ¨¡å‹ä¼˜åŒ–...")

    # åˆå§‹åŒ–SGDä¼˜åŒ–å™¨
    optimizer = optim.SGD(mapping.parameters(), lr=LEARNING_RATE)

    # Best-so-far ç­–ç•¥åˆå§‹åŒ–
    best_loss = float('inf')        # å†å²æœ€ä¼˜æŸå¤±å€¼
    best_mapping_params = None      # æœ€ä¼˜æ˜ å°„å‚æ•°
    best_step = -1                  # æœ€ä¼˜è§£å‡ºç°çš„æ­¥æ•°
    best_metrics = None             # æœ€ä¼˜è§£çš„è¯¦ç»†æŒ‡æ ‡

    # ä¸»ä¼˜åŒ–å¾ªç¯
    for step in range(NUM_OPTIMIZATION_STEPS):
        optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦

        # å‰å‘ä¼ æ’­ï¼šè®¡ç®—æ€§èƒ½æŒ‡æ ‡
        latency, energy, area, mismatch, compat, mapping_invalid_penalty, penalty = perf_model(
            graph=graph,
            hw_params=hw_params,
            mapping=mapping,
            fusion_params=None
        )

        # è®¡ç®—æ€»æŸå¤±ï¼šæ€§èƒ½æŸå¤± + æ˜ å°„æ— æ•ˆæƒ©ç½š
        loss = (latency * energy) + MAPPING_PENALTY_WEIGHT * mapping_invalid_penalty
        current_loss = loss.item()
        
        # Best-so-far ç­–ç•¥ï¼šæ£€æŸ¥å¹¶æ›´æ–°æœ€ä¼˜è§£
        if current_loss < best_loss:
            best_loss = current_loss
            best_step = step
            # æ·±æ‹·è´å½“å‰æœ€ä¼˜çš„æ˜ å°„å‚æ•°
            best_mapping_params = {name: param.data.clone() for name, param in mapping.named_parameters()}
            # ä¿å­˜æœ€ä¼˜è§£çš„è¯¦ç»†æŒ‡æ ‡
            best_metrics = {
                'latency': latency.item(),
                'energy': energy.item(),
                'area': area.item(),
                'mapping_invalid_penalty': mapping_invalid_penalty.item(),
                'penalty': penalty.item()
            }
        
        # æ‰“å°å½“å‰æ­¥éª¤çš„ä¼˜åŒ–ä¿¡æ¯
        print_optimization_step(step, latency, energy, penalty, loss, current_loss, best_loss)
        
        # åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦
        loss.backward()
        
        # æ‰“å°å‚æ•°æ¢¯åº¦å’Œæ›´æ–°ä¿¡æ¯
        print_parameter_gradients(mapping, LEARNING_RATE)
        
        # æ‰§è¡Œå‚æ•°æ›´æ–°
        optimizer.step()
        
        # æ‰“å°æ›´æ–°åçš„mappingå‚æ•°
        print_mapping_parameters(mapping, f"Step {step+1} - æ›´æ–°åçš„Mappingå‚æ•°")

    # ä¼˜åŒ–ç»“æŸåçš„æœ€ä¼˜è§£æ¢å¤
    if best_mapping_params is not None:
        # æ‰“å°æœ€ä¼˜è§£æ‘˜è¦
        print_best_solution_summary(best_step, best_loss, best_metrics)
        
        # æ¢å¤æœ€ä¼˜å‚æ•°åˆ°mappingå¯¹è±¡
        for name, param in mapping.named_parameters():
            param.data.copy_(best_mapping_params[name])
        
        print(f"\nâœ… å·²å°†æœ€ä¼˜è§£å‚æ•°æ¢å¤åˆ°mappingå¯¹è±¡ä¸­")
        
        # æ‰“å°æœ€ä¼˜è§£çš„è¯¦ç»†å‚æ•°ä¿¡æ¯
        print_mapping_parameters(mapping, "ğŸ† æœ€ä¼˜è§£çš„è¯¦ç»†å‚æ•°", show_projected=True)
    else:
        print(f"\nâš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æœ€ä¼˜è§£")

# =============== ä¸»ç¨‹åºå…¥å£ ===============
def main():
    """
    ä¸»ç¨‹åºå…¥å£å‡½æ•°
    
    æ‰§è¡Œå®Œæ•´çš„æ€§èƒ½æ¨¡å‹ä¼˜åŒ–æµç¨‹ï¼š
    1. åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶
    2. è¿è¡Œä¼˜åŒ–å¾ªç¯
    3. è¾“å‡ºæœ€ç»ˆç»“æœ
    """
    print("ğŸ¯ DOSA æ€§èƒ½æ¨¡å‹ä¼˜åŒ–å¼€å§‹")
    print("=" * 70)
    
    # åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶
    config, perf_model, mapping, hw_params, graph = initialize_system()
    
    # è¿è¡Œä¼˜åŒ–
    run_optimization(perf_model, mapping, hw_params, graph)
    
    print("\nğŸ‰ DOSA æ€§èƒ½æ¨¡å‹ä¼˜åŒ–å®Œæˆ")
    print("=" * 70)

if __name__ == "__main__":
    main()
